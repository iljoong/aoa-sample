{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MNIST Sample\n",
    "\n",
    "Keras MNIST sample\n",
    "\n",
    "- dataset: http://yann.lecun.com/exdb/mnist/\n",
    "- source reference: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "\n",
    "This sample demonstrates:\n",
    "\n",
    "- basic CNN models and how it is constructed\n",
    "- interactive test using mouse control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "save_dir = os.path.abspath('../models')\n",
    "model_name = 'sample_mnist_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# data preparation\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.2546 - acc: 0.9213 - val_loss: 0.0584 - val_acc: 0.9805\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0869 - acc: 0.9739 - val_loss: 0.0407 - val_acc: 0.9867\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0662 - acc: 0.9801 - val_loss: 0.0397 - val_acc: 0.9870\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0538 - acc: 0.9835 - val_loss: 0.0335 - val_acc: 0.9885\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0479 - acc: 0.9856 - val_loss: 0.0304 - val_acc: 0.9900\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0429 - acc: 0.9871 - val_loss: 0.0276 - val_acc: 0.9905\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0381 - acc: 0.9884 - val_loss: 0.0279 - val_acc: 0.9912\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0349 - acc: 0.9899 - val_loss: 0.0286 - val_acc: 0.9920\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0310 - acc: 0.9901 - val_loss: 0.0273 - val_acc: 0.9918\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0284 - acc: 0.9912 - val_loss: 0.0247 - val_acc: 0.9918\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0280 - acc: 0.9915 - val_loss: 0.0259 - val_acc: 0.9922\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0263 - acc: 0.9922 - val_loss: 0.0265 - val_acc: 0.9914\n",
      "Test loss: 0.02652348646684841\n",
      "Test accuracy: 0.9914\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\n",
    "# save model\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load model\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffb44049f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_sample = 4002\n",
    "plt.imshow(x_train[test_sample].reshape(28,28), cmap=\"gray_r\")\n",
    "#plt.axis('off')\n",
    "print(y_train[test_sample], np.argmax( y_train[test_sample] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n",
      "[[9.9999976e-01 7.8406731e-10 1.1975330e-07 5.2105140e-11 3.7690057e-10\n",
      "  1.8686422e-10 1.9191010e-08 4.6106832e-10 2.5051659e-09 6.5057840e-08]] 0\n"
     ]
    }
   ],
   "source": [
    "x = x_train[test_sample]\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print(x.shape) # (1, 28, 28)\n",
    "y_pred = model.predict(x)\n",
    "print(y_pred, np.argmax( y_pred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Evaluation\n",
    "\n",
    "Use interactive input to evalueate MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing = False # true if mouse is pressed\n",
    "ix,iy = -1,-1\n",
    "\n",
    "# mouse callback function\n",
    "def draw_circle(event,x,y,flags,param):\n",
    "    global ix,iy,drawing,mode\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ix,iy = x,y\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing == True:\n",
    "            cv2.circle(img,(x,y),7,(255, 255, 255),-1)\n",
    "\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        cv2.circle(img,(x,y),7,(255, 255, 255),-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# draw number and 'q' to finish\n",
    "\n",
    "img = np.full((240, 240, 3), 0, np.uint8) #img = np.zeros((240,240,3), np.uint8)\n",
    "cv2.namedWindow('image')\n",
    "cv2.setMouseCallback('image',draw_circle)\n",
    "\n",
    "while(1):\n",
    "    cv2.imshow('image',img)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# resize - http://opencv-python.readthedocs.io/en/latest/doc/10.imageTransformation/imageTransformation.html\n",
    "dimg = cv2.resize(gray_image, (28, 28), interpolation = cv2.INTER_AREA)\n",
    "testx = dimg.astype(np.float32)\n",
    "plt.imshow(dimg, cmap=\"gray_r\")\n",
    "\n",
    "# dimension - https://stackoverflow.com/questions/49057149/expected-conv2d-1-input-to-have-shape-28-28-1-but-got-array-with-shape-1-2\n",
    "x = np.expand_dims(testx, axis=0)\n",
    "#print(batch.shape) # (1, 28, 28)\n",
    "x /= 255\n",
    "x = np.expand_dims(x, axis=3)\n",
    "#print(batch.shape) # (1, 28, 28, 1)\n",
    "\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "print(y_pred)\n",
    "print(np.argmax(y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
